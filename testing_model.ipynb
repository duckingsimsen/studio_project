{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1ecfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¡œê·¸ì¸ëœ ì‚¬ìš©ì ì •ë³´:\n",
      "â–¶ Username: duckingsimsen\n",
      "â–¶ Email: None\n",
      "â–¶ Gated models access: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# ë‹¹ì‹ ì˜ Hugging Face í† í°ì„ ì…ë ¥í•˜ì„¸ìš”\n",
    "token = \"REMOVEDaQHidcchTGxfXYtmLGJGoXbdoVUUmhYIms\"  # ì˜ˆ: REMOVEDabc123...\n",
    "\n",
    "# Hugging Face ì‚¬ìš©ì ì •ë³´ API\n",
    "url = \"https://huggingface.co/api/whoami-v2\"\n",
    "\n",
    "# ì¸ì¦ í—¤ë” ì¶”ê°€\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\"\n",
    "}\n",
    "\n",
    "# API ìš”ì²­\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "if response.status_code == 200:\n",
    "    user_info = response.json()\n",
    "    print(\"âœ… ë¡œê·¸ì¸ëœ ì‚¬ìš©ì ì •ë³´:\")\n",
    "    print(f\"â–¶ Username: {user_info.get('name')}\")\n",
    "    print(f\"â–¶ Email: {user_info.get('email')}\")\n",
    "    print(f\"â–¶ Gated models access: {user_info.get('gatedRepoAccess', [])}\")\n",
    "else:\n",
    "    print(f\"âŒ ì¸ì¦ ì‹¤íŒ¨ (HTTP {response.status_code}): {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500b3422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\studio_class\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:12<00:00,  3.15s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "\n",
      "your helpful chat botuser\n",
      "\n",
      "ì•ˆë…• ë„ˆì— ëŒ€í•´ì„œ ì†Œê°œí•´ì¤˜.assistant\n",
      "\n",
      "ğŸ˜Š\n",
      "\n",
      "Nice to meet you! My name is LLaMA, I'm a helpful chatbot designed to assist and converse with humans. I'm a large language model, trained on a massive dataset of text from the internet, which allows me to understand and respond to a wide range of questions and topics.\n",
      "\n",
      "I'm here to help you with any questions or tasks you may have. Whether you need assistance with a specific problem, want to chat about a particular topic, or just need someone to talk to, I'm here to listen and help in any way I can.\n",
      "\n",
      "I'm a bit like a conversational AI, which means I can understand natural language and respond accordingly. I can provide information on a wide range of topics, from science and history to entertainment and culture. I can also help with language-related tasks, such as language translation, grammar correction, and text summarization.\n",
      "\n",
      "So, what's on your mind? Do you have a specific question or topic you'd like to discuss? I'm all ears (or rather, all text)! ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "REMOVEDtoken = \"REMOVEDaQHidcchTGxfXYtmLGJGoXbdoVUUmhYIms\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    token = REMOVEDtoken,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"your helpful chat bot\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•ˆë…• ë„ˆì— ëŒ€í•´ì„œ ì†Œê°œí•´ì¤˜.\"}\n",
    "]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)   \n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=8192,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e602d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pytz\n",
    "import torch\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    # 1. CPU ë©”ëª¨ë¦¬ì—ì„œ ë¶ˆí•„ìš”í•œ ê°ì²´ ìˆ˜ì§‘\n",
    "    gc.collect()\n",
    "    # 2. PyTorch ìºì‹œ ë¹„ìš°ê¸°\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()  # CUDA IPC ê³µìœ  ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    print(\"âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "clear_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studio_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
