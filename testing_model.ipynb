{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1ecfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 로그인된 사용자 정보:\n",
      "▶ Username: duckingsimsen\n",
      "▶ Email: None\n",
      "▶ Gated models access: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 당신의 Hugging Face 토큰을 입력하세요\n",
    "token = \"REMOVEDaQHidcchTGxfXYtmLGJGoXbdoVUUmhYIms\"  # 예: REMOVEDabc123...\n",
    "\n",
    "# Hugging Face 사용자 정보 API\n",
    "url = \"https://huggingface.co/api/whoami-v2\"\n",
    "\n",
    "# 인증 헤더 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\"\n",
    "}\n",
    "\n",
    "# API 요청\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 결과 출력\n",
    "if response.status_code == 200:\n",
    "    user_info = response.json()\n",
    "    print(\"✅ 로그인된 사용자 정보:\")\n",
    "    print(f\"▶ Username: {user_info.get('name')}\")\n",
    "    print(f\"▶ Email: {user_info.get('email')}\")\n",
    "    print(f\"▶ Gated models access: {user_info.get('gatedRepoAccess', [])}\")\n",
    "else:\n",
    "    print(f\"❌ 인증 실패 (HTTP {response.status_code}): {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500b3422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\studio_class\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.15s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "\n",
      "your helpful chat botuser\n",
      "\n",
      "안녕 너에 대해서 소개해줘.assistant\n",
      "\n",
      "😊\n",
      "\n",
      "Nice to meet you! My name is LLaMA, I'm a helpful chatbot designed to assist and converse with humans. I'm a large language model, trained on a massive dataset of text from the internet, which allows me to understand and respond to a wide range of questions and topics.\n",
      "\n",
      "I'm here to help you with any questions or tasks you may have. Whether you need assistance with a specific problem, want to chat about a particular topic, or just need someone to talk to, I'm here to listen and help in any way I can.\n",
      "\n",
      "I'm a bit like a conversational AI, which means I can understand natural language and respond accordingly. I can provide information on a wide range of topics, from science and history to entertainment and culture. I can also help with language-related tasks, such as language translation, grammar correction, and text summarization.\n",
      "\n",
      "So, what's on your mind? Do you have a specific question or topic you'd like to discuss? I'm all ears (or rather, all text)! 😊\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "REMOVEDtoken = \"REMOVEDaQHidcchTGxfXYtmLGJGoXbdoVUUmhYIms\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    token = REMOVEDtoken,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"your helpful chat bot\"},\n",
    "    {\"role\": \"user\", \"content\": \"안녕 너에 대해서 소개해줘.\"}\n",
    "]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)   \n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=8192,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e602d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU 메모리 정리 완료\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pytz\n",
    "import torch\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    # 1. CPU 메모리에서 불필요한 객체 수집\n",
    "    gc.collect()\n",
    "    # 2. PyTorch 캐시 비우기\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()  # CUDA IPC 공유 메모리 정리\n",
    "    print(\"✅ GPU 메모리 정리 완료\")\n",
    "\n",
    "clear_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studio_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
